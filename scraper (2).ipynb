{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhy_AULrGAha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! apt-get install poppler-utils\n",
        "! sudo apt install tesseract-ocr\n",
        "! sudo apt-get install tesseract-ocr\n",
        "! pip install pdf2image\n",
        "! pip install pytesseract\n",
        "! wget \"https://nss.org.sg/report/1eebac90-8Avi_2005_19_2.pdf\" # DONE\n",
        "! wget \"https://nss.org.sg/report/8baffd7a-9Avi_2005_19_1.pdf\" # DONE\n",
        "! wget \"https://nss.org.sg/report/481b0c11-eAvi_2005_19_3.pdf\" # DONE\n",
        "! wget \"https://nss.org.sg/report/3d428c84-fAvi_2006_19_4.pdf\" # DONE\n",
        "! pip install word2number\n",
        "% load_ext google.colab.data_table\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_treebank_pos_tagger') \n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "df_in = pd.read_csv(\"/content/gdrive/My Drive/df (10).csv\")\n",
        "datafile = pd.read_csv(\"/content/gdrive/My Drive/ebird-checklist.csv\", encoding= 'unicode_escape')\n",
        "cachedsci = pickle.load(open(\"/content/gdrive/My Drive/cachedsci.pkl\", \"rb\"))\n",
        "cachedeng = pickle.load(open(\"/content/gdrive/My Drive/cachedeng.pk\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9a5p4nqGPkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pdf2image import convert_from_path, convert_from_bytes\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import partial\n",
        "\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "images = convert_from_bytes(open('3d428c84-fAvi_2006_19_4.pdf', 'rb').read())\n",
        "\n",
        "text = ''\n",
        "\n",
        "for i in tqdm(images[4:30]):\n",
        "  open_cv_image = np.array(i) \n",
        "  img = open_cv_image[:, :, ::-1].copy()\n",
        "  img = cv2.resize(img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  # gray = cv2.medianBlur(gray, 3)\n",
        "  gray, img_bin = cv2.threshold(gray,128,255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "  gray = cv2.bitwise_not(img_bin)\n",
        "\n",
        "  kernel = np.ones((2, 1), np.uint8)\n",
        "  img = cv2.erode(gray, kernel, iterations=1)\n",
        "  img = cv2.dilate(img, kernel, iterations=1)\n",
        "  tt = pytesseract.image_to_string(cv2.bitwise_not(img))\n",
        "  text += (tt.replace(\"/\\n\",\"/\") + \"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fkSeEGcNgbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import pandas as pd\n",
        "import nltk.tag, nltk.data\n",
        "import copy\n",
        "from word2number import w2n\n",
        "\n",
        "res = re.compile(r'^([A-Z -]){5,}.*?[A-Z]?[a-z ]*$')\n",
        "splitter = re.compile(r'([({]a?l?l?[A-Z/ ]*[)/}]|\\(sev obs\\))')\n",
        "date = re.compile(r'[0-9]{1,2} ?/ ?[0-9]{1,2}')\n",
        "\n",
        "class Sighting:\n",
        "  def __init__(self, species, date, count, person, location, string=None):\n",
        "    self.person = person\n",
        "    self.date = date\n",
        "    self.species = species\n",
        "    self.count = count\n",
        "    self.string = string\n",
        "    self.location = location\n",
        "    self.data = [species, date, count, person, location, string]\n",
        "  def __str__(self):\n",
        "    return f\"SPECIES: {self.species}\\nDATE: {self.date}\\nCOUNT: {self.count}\\nPERSON: {self.person}\\nLOCATION: {self.location}\\nCOMMENTS: {self.string}\"\n",
        "  def join(self, other):\n",
        "    data2 = zip(self.data, other.data)\n",
        "    datanew = []\n",
        "    n0 = 0\n",
        "    for a, b in data2:\n",
        "      if n0 == 3:\n",
        "        n0 += 1\n",
        "        if a in ['', 0] and b not in ['', 0]:\n",
        "          datanew.append([b, b])\n",
        "        continue\n",
        "      if a in ['', 0] and b not in ['', 0]:\n",
        "        datanew.append([b, b])\n",
        "      elif a not in ['', 0] and b in ['', 0]:\n",
        "        datanew.append([a, a])\n",
        "      else:\n",
        "        datanew.append([a, b])\n",
        "      n0 += 1\n",
        "    return (Sighting(*[a for a, b in datanew]), Sighting(*[b for a, b in datanew]))\n",
        "\n",
        "def make_df(sightings):\n",
        "  d = {\n",
        "      \"Species\": [i.species for i in sightings],\n",
        "      \"Date\": [i.date for i in sightings],\n",
        "      \"Count\": [i.count for i in sightings],\n",
        "      \"Observer\": [i.person for i in sightings],\n",
        "      \"Location\": [i.location for i in sightings],\n",
        "      #\"Comments\": [i.string for i in sightings]\n",
        "  }\n",
        "  return pd.DataFrame.from_dict(d)\n",
        "\n",
        "spdata = {}\n",
        "spdata2 = []\n",
        "\n",
        "latest = ''\n",
        "\n",
        "for t in text.split(\"\\n\"):\n",
        "  if re.match(res, t)!=None:\n",
        "    if t not in spdata.keys():\n",
        "      spdata[t] = ''\n",
        "    latest = t\n",
        "  elif latest != '':\n",
        "    spdata[latest] += (' ' + t)\n",
        "\n",
        "n = 0\n",
        "\n",
        "for k, i in list(spdata.items()):\n",
        "  n += 1\n",
        "  if len(date.findall(i)) <= len(splitter.findall(i)):\n",
        "    s = splitter.split(i)\n",
        "    dates = (date.findall(i))\n",
        "    li = list(zip(s[::2], s[1::2], dates))\n",
        "    li = [(a.replace(c,''), b, c) for a, b, c in li ]\n",
        "  else:\n",
        "    s = date.split(i)\n",
        "    dates = (date.findall(i))\n",
        "    obs = splitter.findall(i)\n",
        "    li = list(zip(s[::2], obs, dates))\n",
        "    li = [(a.replace(c,''), b, c) for a, b, c in li ]\n",
        "  for string, person, d in li:\n",
        "    original = copy.deepcopy(string)\n",
        "    string = string.rstrip(', ').lstrip(', ')\n",
        "    for repl in [\"A pair\", \"a pair\", \"1 pair\", \"pair\", \"Pair\", \"1 male and 1 female\"]:\n",
        "      string = string.replace(repl, \"2\")\n",
        "    string = string.replace(\"Singapore Avifauna\", '')\n",
        "    for repl in [\"a juvenile\", \"a female\", \"a male\", \"a summer male\", \"another\", \"\\|\", \"an immature\", '\\[']:\n",
        "      string = re.compile(repl, re.IGNORECASE).sub('1', string)\n",
        "    if all([word in [\"and\", \",\", ''] for word in string.split(\" \")]) or len(string) == 0:\n",
        "      spdata2.append(Sighting(k, d, 0, person, '', string=string))\n",
        "      continue\n",
        "    location = []\n",
        "    count = []\n",
        "    wtk = word_tokenize(string)\n",
        "    pos_tag1 = pos_tag(wtk)\n",
        "    chunk = ne_chunk(pos_tag1)\n",
        "    for ix in chunk.subtrees(filter=lambda x: x.label() in [\"PERSON\", \"ORGANIZATION\", \"FACILITY\", \"LOCATION\"]):\n",
        "      location.append(\" \".join([a for a, b in ix]))\n",
        "    if len(location) == 0:\n",
        "      for ix in wtk:\n",
        "        if ix[0].isupper():\n",
        "          location.append(ix)\n",
        "      location = [\" \".join(location)]\n",
        "    location = [l for l in location if l not in [\"Sungei Puaka\", \"Kg Melayu\", \"Nesting Report\", \"Ficus\", \"Trail\", \"African Tulip Tree\"]]\n",
        "    for ix in chunk:\n",
        "      if len(ix) > 1 and (ix[1] == \"CD\") and '/' not in ix[0]:\n",
        "        count.append(ix[0])\n",
        "    if \"Lane 2\" in string:\n",
        "      count.remove('2')\n",
        "    string = re.compile(\"[0-9]+ ?meters\", re.IGNORECASE).sub('', string)\n",
        "    string = re.compile(\"[0-9]+ ?metres\", re.IGNORECASE).sub('', string)\n",
        "    if len(count) != 1:\n",
        "      while True:\n",
        "        inp = input(f\"COUNT for {string.strip()}      {n}/{len(spdata.items())}\\n\").strip()\n",
        "        if inp == 'skip': \n",
        "          break\n",
        "        try: \n",
        "          if inp == '':\n",
        "            count = 0\n",
        "            break\n",
        "          count1 = int(inp)\n",
        "          break\n",
        "        except:\n",
        "          if inp == 'more':\n",
        "            print(k, i)\n",
        "          continue\n",
        "    else:\n",
        "      inp = count[0].replace('.', '')\n",
        "    if count != 0 and inp != 'skip': \n",
        "      try:\n",
        "        count = int(inp)\n",
        "      except ValueError:\n",
        "        try:\n",
        "          count = w2n.word_to_num(inp)\n",
        "        except ValueError:\n",
        "          while True:\n",
        "            inp = input(f\"COUNT for {string.strip()}      {n}/{len(spdata.items())}\\n\").strip()\n",
        "            if inp == 'skip': \n",
        "              break\n",
        "            try: \n",
        "              if inp == '':\n",
        "                count = 0\n",
        "                break\n",
        "              count1 = int(inp)\n",
        "              break\n",
        "            except:\n",
        "              if inp == 'more':\n",
        "                print(k, i)\n",
        "              continue\n",
        "    if len(location) > 1:\n",
        "      while True:\n",
        "        inp = input(f\"LOCATION for {string.strip()}; options: {location}      {n}/{len(spdata.items())}\\n\").strip()\n",
        "        if inp == 'skip': \n",
        "          break\n",
        "        try: \n",
        "          if inp == '':\n",
        "            location = ''\n",
        "            break\n",
        "          location1 = location[int(inp)]\n",
        "          break\n",
        "        except:\n",
        "          if inp == 'more':\n",
        "            print(k, i)\n",
        "          continue\n",
        "    else:\n",
        "      if inp != 'skip':\n",
        "        inp = 0\n",
        "    try:\n",
        "      if location != '' and inp != 'skip':\n",
        "        location = location[int(inp)]\n",
        "    except ValueError:\n",
        "      location = \" \".join([ location[i] for i in map(int, inp.split(',')) ])\n",
        "    if inp != 'skip':\n",
        "      spdata2.append(Sighting(k, d, count, person, location, string=string))\n",
        "    else:\n",
        "      print(\"skipped this one\")\n",
        "\n",
        "for x in range(1, len(spdata2)):\n",
        "  spdata2[x], spdata2[x-1] = spdata2[x].join(spdata2[x-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPwoBxI-xInP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "import editdistance\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "import pickle\n",
        "from google.colab import data_table\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# cachedeng = {k: list(datafile.loc[datafile[\"scientific name\"] == v][\"English name\"])[0] for k, v in cachedsci.items()}\n",
        "# print(cachedeng)\n",
        "\n",
        "# a_file = open(\"cachedeng.pkl\", \"wb\")\n",
        "# pickle.dump(cachedeng, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "overrides = {\n",
        "    \"Lonchura molucca\": \"Lonchura atricapilla\",\n",
        "    \"Black-faced Munia\": \"Black-headed Munia\",\n",
        "    \"Nilgiri Flowerpecker\": \"Plain Flowerpecker\",\n",
        "    \"Dicaeum concolor\": \"Dicaeum minullum\",\n",
        "    \"Collared Scops-Owl\": \"Sunda Scops-Owl\",\n",
        "    \"Otus lettia\": \"Otus lempiji\"\n",
        "}\n",
        "\n",
        "cachedsci = {k: v if v not in overrides.keys() else overrides[v] for (k, v) in cachedsci.items()}\n",
        "cachedeng = {k: v if v not in overrides.keys() else overrides[v] for (k, v) in cachedeng.items()}\n",
        "\n",
        "done = 0\n",
        "\n",
        "def get(spname):\n",
        "  global done\n",
        "  datafile2 = datafile.copy(deep=True)\n",
        "  text = spname.lower()\n",
        "  datafile2[\"Distance\"] = datafile2[\"Pair\"].apply(lambda i: editdistance.eval(text, i))\n",
        "  datafile2 = datafile2.sort_values(by='Distance')\n",
        "  vsci = list(datafile2[\"scientific name\"])[0]\n",
        "  veng = list(datafile2[\"English name\"])[0]\n",
        "  score = list(datafile2[\"Distance\"])[0]\n",
        "  if score > 5:\n",
        "    try:\n",
        "      print(text, done)\n",
        "      n = 3\n",
        "      display(datafile2.head(3))\n",
        "      inpu = input(\"Confirm?\").strip()\n",
        "      while inpu == 'more':\n",
        "        n += 5\n",
        "        display(datafile2.head(n))\n",
        "        inpu = input(\"Confirm?\").strip()\n",
        "      if inpu != '':\n",
        "        vsci = list(datafile2[\"scientific name\"])[int(inpu)]\n",
        "        veng = list(datafile2[\"English name\"])[int(inpu)]\n",
        "        inpu2 = input(\"Confirm again\").strip()\n",
        "        if inpu != '':\n",
        "          vsci = list(datafile2[\"scientific name\"])[int(inpu2)]\n",
        "          veng = list(datafile2[\"English name\"])[int(inpu2)]\n",
        "    except ValueError:\n",
        "      text2 = input(\"Species name\").strip()\n",
        "      datafile2[\"Distance\"] = datafile2[\"Pair\"].apply(lambda i: editdistance.eval(text2, i))\n",
        "      datafile2 = datafile2.sort_values(by='Distance')\n",
        "      vsci = list(datafile2[\"scientific name\"])[0]\n",
        "      veng = list(datafile2[\"English name\"])[0]\n",
        "      score = list(datafile2[\"Distance\"])[0]\n",
        "      if score > 5:\n",
        "          print(text2)\n",
        "          n = 3\n",
        "          display(datafile2.head(3))\n",
        "          inpu = input(\"Confirm?\").strip()\n",
        "          while inpu == 'more':\n",
        "            n += 5\n",
        "            display(datafile2.head(n))\n",
        "            inpu = input(\"Confirm?\").strip()\n",
        "          if inpu != '':\n",
        "            vsci = list(datafile2[\"scientific name\"])[int(inpu)]\n",
        "            veng = list(datafile2[\"English name\"])[int(inpu)]\n",
        "            inpu2 = input(\"Confirm again\").strip()\n",
        "            if inpu != '':\n",
        "              vsci = list(datafile2[\"scientific name\"])[int(inpu2)]\n",
        "              veng = list(datafile2[\"English name\"])[int(inpu2)]\n",
        "        \n",
        "  cachedsci[text] = vsci\n",
        "  cachedeng[text] = veng\n",
        "  clear_output()\n",
        "  return (vsci, veng)\n",
        "\n",
        "def optimum(spname, scieng):\n",
        "  global done\n",
        "  done += 1\n",
        "  global cachedsci, cachedeng\n",
        "  try:\n",
        "    if scieng == 'sci':\n",
        "      return cachedsci[spname.lower()]\n",
        "    if scieng == 'eng':\n",
        "      return cachedeng[spname.lower()]\n",
        "  except KeyError:\n",
        "    if scieng == 'sci': \n",
        "      return get(spname)[0]\n",
        "    if scieng == 'eng': \n",
        "      return get(spname)[1]\n",
        "\n",
        "made = make_df([i for i in spdata2 if type(i) == Sighting])\n",
        "made[\"Comments\"] = made[\"Species\"].apply(lambda i: spdata[i].strip())\n",
        "\n",
        "df = pd.concat([df_in, made])\n",
        "\n",
        "def dt(row):\n",
        "  d = row[\"Date\"]\n",
        "  if type(d) == str:\n",
        "    try:\n",
        "      if d.count('/') == 1:\n",
        "        return datetime(year=2005,month=int(d.split(\"/\")[1]),day=int(d.split(\"/\")[0]))\n",
        "      elif d.count('/') == 2:\n",
        "        return datetime(year=int(d.split(\"/\")[2]),month=int(d.split(\"/\")[0]),day=int(d.split(\"/\")[1]))\n",
        "      if d.count('-') == 2:\n",
        "        return datetime(year=int(d.split(\"-\")[0]),month=int(d.split(\"-\")[1]),day=int(d.split(\"-\")[2]))\n",
        "    except:\n",
        "      return None\n",
        "  else:\n",
        "    return d\n",
        "\n",
        "df[\"Date\"] = df.apply(dt,axis=1)\n",
        "\n",
        "datafile[\"Pair\"] = datafile[\"Pair\"].apply(lambda i: i.lower())\n",
        "df[\"English name\"] = df[\"Species\"].progress_apply(lambda i: optimum(i, 'eng'))\n",
        "df[\"Scientific name\"] = df[\"Species\"].apply(lambda i: optimum(i, 'sci'))\n",
        "df = df[~(df[\"Date\"]==None)]\n",
        "\n",
        "a_file = open(\"cachedsci.pkl\", \"wb\")\n",
        "pickle.dump(cachedsci, a_file)\n",
        "a_file.close()\n",
        "\n",
        "a_file2 = open(\"cachedeng.pkl\", \"wb\")\n",
        "pickle.dump(cachedeng, a_file2)\n",
        "a_file2.close()\n",
        "\n",
        "df[\"Count\"] = df[\"Count\"].apply(lambda i: i if type(i) == int or str(i).isdigit() else 'X')\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "cols = [\"English name\", \"Scientific name\", \"Date\", \"Count\", \"Observer\", \"Location\", \"Comments\"]\n",
        "cols2 = [\"Species\", \"Date\", \"Count\", \"Observer\", \"Location\", \"Comments\"]\n",
        "\n",
        "df_display = df[cols]\n",
        "\n",
        "df_in = df.copy(deep=True)\n",
        "df[cols2].to_csv(\"df (10).csv\", index=False)\n",
        "files.download(\"df (10).csv\")\n",
        "\n",
        "df_display.to_csv(\"output.csv\", index=False)\n",
        "files.download(\"output.csv\")\n",
        "\n",
        "data_table.DataTable(df_display, include_index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IP8y8COYV9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a79339c-bc21-4201-c679-c7ca808ea458"
      },
      "source": [
        "% cd ..\n",
        "% cp \"/content/df (10).csv\" -r \"/content/gdrive/My Drive/\"\n",
        "% cp \"/content/output.csv\" -r \"/content/gdrive/My Drive/\"\n",
        "% cp \"/content/cachedeng.pkl\" -r \"/content/gdrive/My Drive/\"\n",
        "% cp \"/content/cachedsci.pkl\" -r \"/content/gdrive/My Drive/\""
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}